 % The main file for CAMP reports
 % Don't put any content in here. 
 % Don't even include content files by using \input or \inlcude. 
 % Put your content to TEXT.TEX or include it there using \input.
 % Uses:
 %		SETTINGS.TEX	contains the settings for this document
 %		COMMANDS.TEX	contains commands which can be used while writing
 %		INFO.TEX			contains the author, title and so on for the cover
 %		COVER.TEX			formats\documentclass[10pt]{?} the front cover of the document
 %		ABSTRACT.TEX	contains the abstract to be included (if needed)
 %		TEXT.TEX			contains the actual content of the document
 %		BIB.BIB				containt the BibTeX entries for the document
 
 
%% Draft document mode
%% Final document
\documentclass[11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR12mm,DIV13]{scrbook}


%\documentclass[11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR20mm,DIV10]{scrbook}

% KOMA-Optionen:
%  bibtotoc: include bibliography in table of contents
%  idxtotoc: include index in table of contents
%  headsepline: use horizontalline under heading
%  BCOR: binding correcion (Bindungskorrektur) (e.g.: BCOR5mm)
%  DIV: Number of sheet sections (used for layout) (e.g.: DIV12) 


% include title and author information for the cover
\input{components/info}

% include settings
\input{components/settings}

% include commands
\input{components/commands}


%\makeindex
	%% inter line spacing
%\linespread{1.0}

\makeglossary

\begin{document}

	\frontmatter
	
	
	\input{components/cover}
%	\clearemptydoublepage
	
	%\input{components/titlepage}
	
	
%	\input{components/cover_maschmeyer}
	\clearemptydoublepage
	
	\input{components/titlepage}
	
	
	\input{components/disclaimer}
	
	\input{components/acknowledgements}
	
	\input{components/abstract}

	\tableofcontents
  
 % \input{components/outline}

	\mainmatter
	
	



\chapter{Introduction}
\label{chap:introduction}

Whenever we see our friends posting pictures on Facebook or Instagram, we generally like them or comment on them. Whenever we feel like sharing our thoughts, we either update status on Facebook or just tweet about it. If we need some relevant information, we just google it. The amount of data generated in such a fashion has to be stored somewhere. Companies like Facebook stores 500 TB of data each day\cite{daniel:datastats}, including 2.7 billion likes and 300 million photos. As of 2012, Facebook already has 100 petabyte of photos\cite{daniel:datastats}. Google, on the other hand processes 3.5 billion request per day \cite{daniel:datastats}. In Early 2000s, where there were less data shared on social media, data were stored in a relational database. Relational database were designed in such a fashion to store small amount of data and maintain integrity between them\cite{matt:rdb}. The amount of information we share on social media is expected to grow from 4.4 zettabytes in 2013 to 44 zettabytes in 2020(1 zettabyte is 1 trillion gigabytes)\cite{matt:rdb}. The scaling in RDBMS depends on adding more powerful CPU's and memory, i.e. only the vertical scaling is possible which is rather expensive.  One of the advantages of big data storage system is that it can be scaled horizontally and is also useful for storing unstructured or semi structured data. 

HBase is an open source sortedMap Datastore from Apache Software Foundation which is used as a database to store huge volume of data. HBase supports horizontal scalability, i.e. parts of a table can be put on several machines. This way a table is broken down to multiple pieces, thus making computation really fast. But when we are talking about petabytes of data, scanning each part of table for a single user query is still considered to be expensive in terms of processing time. There are several techniques to reduce this effort, but we will be talking about $Materialized$ $Views$ approach. 


%In Chapter \ref{chap:background}, we analyse the fundamentals of views and view maintenance. In Chapter \ref{chap:relatedwork}, we present research, that is related to this thesis. In Chapter \ref{chap:analysis}, we define the requirements of the View Maintenance System and discuss possible alternatives.  In Chapter \ref{chap:architecture}, we align the architecture of the system and define its functionalities. In Chapter \ref{chap:viewconsistency}, we discuss threats to consistency and apply consistency techniques. In Chapter \ref{chap:loadbalancing}, we show how load balancing can be accomplished in the View Maintenance System. In Chapter \ref{chap:failuredetection}, we take counter measures to component failure. In Chapter \ref{chap:implementation}, we show challenges of the implementation. Finally, we evaluate and interpret the behaviour of the system in Chapter \ref{chap:evaluation}.\\%


\chapter{Background}
\label{chap:background}

In this chapter we will first discuss about the fundamentals of $Materialized$ $Views$ and $View$ $Types$. We will further explain about the technologies used widely in today's Distributed Storage Databases. 

\section{Materialized Views}

Materialized views are defined as the database object that stores the result of a query in a table or a disk. Materialized views are widely used for gaining performance advantage, i.e. to speed up query processing time over large datasets. The need for Materialized view addresses the problem of having to query large datasets that often needs joins and aggregations between multiple tables. These kind of queries are very expensive, in terms of execution time and processing power. Materialized views speeds up the query processing time by pre-computing joins and aggregations prior to execution and stores these results in a table or disk\cite{materializedview:oracle}. 

\section{View Maintenance}
Once the Materialized views are created, our query is redirected to Materialized View table rather than base table. Whenever there is an update in the base table, the Materialized View table also has to be updated accordingly. One of the solutions would be recomputing the whole Materialized View from the scratch or using the heuristic of inertia\cite{maintenance:materializedviews} approach i.e. incremental maintenance with respect to the base table.

\section{Incremental Maintenance of Materialized View}
"A view V is considered consistent with the database DB if the evaluation of the view specification S over the database yields the view instance (V = S(DB)). Therefore, when the database DB is updated to DB0 , we need to update the view V to V0 = S(DB0) in order to preserve its consistency"\cite{incrementalmaintenance:materializedviews}. 

Recomputing Materialized view from scratch every time there is an update on base table is expensive. The other approach is to update the part of Materialized view table with respect to the update in Base Table. Our target is to maintain consistency between Materialized views and base table whenever there is an update on the base table.


\subsection{Aggregation}
In Aggregation view type, the data of base table is merged on the basis of a particular key. In our implementation, we've implemented basic aggregation functions like sum, count, min and max. All these operations are carried out based on a particular key. So a unique key has sum, count, min and max operations. Whenever an update is triggered to update value for a particular key in the base table, in this case, count remains same and sum, min and max has to be recalculated. If a delete is triggered for a particular key in the base table, each of the aggregation functions has to be recalculated. 


\subsection{Join and Aggregation}
In Join and Aggregation case, we have at least two base tables. Joins being one of the complex structure itself, incremental view maintenance implementation involves a lot of complex cases. Here, to reduce complexity, we join two base tables on the basis of $key$ to form a new intermediate table. We group all the values of both base tables based on their keys. This way, for any update or delete trigger, the complexity of scanning whole base table is reduced to a single row. In our intermediate table, each of the base table is merged to a column family, join is applied and then result is stored in the view table. 

In the intermediate table, the unique keys from both the base tables act as the row key, both column families from base table are merged in the intermediate table. Now for a particular row key, the values are selected from base table and plotted in the intermediate table. Now join is applied between both column families of a particular row key, and sum of the join is inserted in the view table.  


\subsection{Join and Selection}
Join and Selection case is similar to the Join and Aggregation case, the only difference is instead of applying aggregation function, the join is applied for a particular row key and value is selected and inserted into the view table.


\section{HBase}
\label{sec:hbase}
HBase is an open source sortedMap Datastore from Apache Software Foundation. HBase is modeled after Google's BigTable framework. A basic table structure of HBase consists of Row Key, which is similar to the primary key in relational database table, Column Family and Column Qualifier.  

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{HBase}
	\caption{HBase Table}
	\label{fig:hbasetable}
\end{figure}

\newpage
\section{Coprocessor}
\label{sec:coprocessor}

HBase Coprocessor framework provides library to run user code in the HBase Region Server. The advantage of this framework is that it decreases the communication overhead of transferring the data from HBase region server to the client, thus improving the performance by allowing the real computation to happen in the HBase region server\cite{coprocessor:coprocessor}. There are two types of coprocessor, Observer coprocessor  which acts more like relational database triggers and Endpoint coprocessor that resembles stored procedures of RDBMS\cite{coprocessor:types} 


\subsection{Observer coprocessor}
Observer coprocessor as stated earlier, are more like database triggers that executes our code when certain events occurs. In the figure below, we first try to explain a simple life cycle of put() operation as an example\cite{coprocessor:detail}. Observer coprocessor resides between the client and the HMaster. Observer coprocessor can be triggered after every get(), put() or delete() command. The CoprocessorHost class is responsible for observer registration and execution\cite{coprocessor:detail}.  During the life cycle of events, Observer coprocessor allows us to hook triggers in two stages. The first one is before the occurrence of the event and the other is after the completion of the event. For example, if we want to perform some computations before the occurrence of put event, we can use prePut() method to perform our custom computation. Then the life cycle of put event starts and after the life cycle of put event is completed, we can use postPut() method to perform custom computation. In the figure below, we try to explain the lifecycle of observer coprocessor when a put event is fired\cite{coprocessor:detail}.
\newline
There are four types of Observer Interfaces provided as of HBase version 1.1.3\cite{hbase:essential}.

\begin{enumerate}
	
\item RegionObserver: RegionObserver runs on all the Region of a HBase table. RegionObserver provides hook for data manipulation for events like put(), get() add delete() events. All the data manipulations are done with pre hook and post hook\cite{hbase:essential} such as pre and post observers. For instance, preGetOp() and postGetOp() provides hook for manipulating get request. 

\item RegionServerObserver: Likewise in RegionObserver, RegionServerObserver provides hook for data manipulation for events like merge, commits and rollback. All the data manipulation are done with pre hook and post hook such as preMerge() and postMerge(). 

\item WALObserver: WALObserver interface provides hook for Write-Ahead-Log(WAL)\cite{hbase:essential} related operations. This interface provides only preWALWrite() which is triggered before WALEdit is written to Write-Ahead-Log and postWALWrite() which is triggered after WALEdit is written to a Write-Ahead-Log.

\item MasterObserver: MasterObserver Interface provides hook for data manipulation for DDL events such as table creation, table deletion or table modification\cite{cloudera:instandupg}. For instance, if the secondary indexes needs to be deleted when primary table is deleted, we can use postDeleteTable(). The MasterObserver runs on master node.

\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{PutRequestLifeCycle}
	\caption{Life cycle of put request}
	
\end{figure}

In the figure below, We can see the life-cycle of a put request with observer coprocessor implemented.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{PutRequestWithObserverCoprocessor}
	\caption{Life cycle of put request with observer coprocessor}
	
\end{figure}

\subsection{Endpoint coprocessor}

Endpoint coprocessor are similar to the Stored Procedures in RDBMS. This type of coprocessor is more useful in the scenario where the computation is needed for the whole table and are not provided by observer coprocessor\cite{hbase:coprocessors}. Invoking the endpoint coprocessor is similar to invoking any other commands in HBase from the client\textquotesingle s point of view but the result is based on the code that defines the coprocessor\cite{coprocessor:detail}. The figure below explains the Aggregation example\cite{coprocessor:detail}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{EndPointCoprocessor}
	\caption{EndPoint Coprocessor}
	
\end{figure}

When a request is invoked from a client, an instance of Batch.call() encapsulates the request invocation and the request is forwarded to coprocessorExec() method of HTableInterface. Then the coprocessorExec() handles the request invocation and distributes the request to all the Regions of the RegionServer. Assuming that no interruptions occurs and all the requests are completed, the results is then returned to client and aggregated\cite{coprocessor:detail}.


\newpage
\chapter{Implementation}
In this section we will first discuss about the prerequisite of implementation and then the proposed method for our research.

\section{Prerequisite}
Before we begin with our implementation of coprocessor, there are few steps to load coprocessor into our HBase table. Coprocessor can be loaded to the base tables in two ways: statically and dynamically\cite{loading:coprocessors}. 

\subsection{Static Loading of coprocessor}
We have to define coprocessor properties in a $hbase$-$site.xml$ file inside a \textless property\textgreater \space element followed by \textless name\textgreater \space and a \textless value\textgreater \space sub element. The \textless name\textgreater \space sub element should have one of the followings\cite{hbase:site}:

\begin{enumerate}
	\item hbase.coprocessor.region.classes for RegionObservers and Endpoints coprocessor
	\item hbase.coprocessor.wal.classes for WALObservers
	\item hbase.coprocessor.master.classes for MasterObservers
	
\end{enumerate}

The \textless value\textgreater \space sub-element should contain the full path of the coprocessor implementation class. A typical example for static loading of coprocessor looks as,\newline \newline \textless property\textgreater \newline
\textless name\textgreater hbase.coprocessor.region.classes\textless /name\textgreater \newline
\textless value\textgreater \texttt{org.apache.hbase.HBase\_coprocessor.HBaseCoprocessor}\textless /value\textgreater \newline
\textless /property\textgreater \newline

 If we have multiple classes, then the path in \textless value\textgreater \space sub element should be comma separated. In this setup, the framework will attempt to load all the configured classes, so we have to create a jar with dependencies, for all the classes and place the location of jar to HBase classpath. For that, we have to export /path/to/jar in $hbase$-$env.sh$ file. A typical example for exporting classpath is given below, \newline \newline
 export \texttt{HBASE\_CLASSPATH}='/path/to/jar' \newline
 
Now if HBase is restarted without any errors, we have managed to load system coprocessor successfully. 

\subsection{Static Unloading of coprocesssor}
\begin{enumerate}
	\item Delete entry from $hbase$-$site.xml$
	\item Delete entry for $hbase$-$env.sh$
	\item Restart HBase
\end{enumerate}

\subsection{Dynamic Loading of coprocessor}
In this approach, rather than loading coprocessor to all the tables in a Region, the coprocessor are loaded to specific tables of the region. There are two implementations of loading coprocessor dynamically, from HBase shell or using Java API\cite{hbase:site}.

\subsubsection{Using HBase shell}
\begin{enumerate}
	\item disable table \newline
			\space \space hbase\textgreater disable '\textless \texttt{table\_name}\textgreater'
	\item load coprocessor using the following command \newline
	alter '\textless \texttt{table\_name}\textgreater', \newline 
	METHOD =\textgreater '\textless \texttt{table\_att}\textgreater', 'coprocessor' =\textgreater '/file/to/path\text{\textbar}\newline/source/path/to/impementation/class\text{\textbar}1001\text{\textbar}'
	\newline \newline
	A typical example looks like,\newline \newline
	alter 'BaseTableA', METHOD =\textgreater '\texttt{table\_att}', 'coprocessor' =\textgreater 'file:///home/saroj-gautam/Documents/HBase-coprocessor-0.0.1-SNAPSHOT-jar-with-dependencies.jar\text{\textbar}\texttt \newline {org.apache.hbase.HBase\_coprocessor.HBaseCoprocessor}\text{\textbar}1001\text{\textbar}'
	
	\item enable table \newline
	See if coprocessor is loaded successfully. We can see it by seeing the table properties. \newline 
	hbase\textgreater describe '\textless \texttt{table\_name}\textgreater \space should list the coprocessor under \texttt{TABLE\_ATTRIBUTES.} \newline

\end{enumerate}

In the above scenario, the coprocessor tries to read class information from \texttt{table\_att} property. There are certain arguments separated by pipe (\text{\textbar}). The first argument in the value is the file path to the jar file that contains the implementation class. The second argument contains the full classname of the implemented coprocessor. The last argument represents the execution sequence of registered observers. If this field is left blank, the framework will itself assign a default priority value\cite{hbase:site}.

\subsubsection{Using Java API}
Prior to HBase version 0.96, the coprocessors were loaded in a different way. After HBase version 0.96 and newer, HTableDescriptor class provides addCoprocessor() method that helps to load coprocessor in an easier way. A code snippet\cite{loading:coprocessors} below will give us a basic insight of how coprocessor is loaded dynamically from Java API in older versions and newer versions of HBase.
\newline \newline
\textbf{Older than 0.96} \newline
\lstset{language=Java}
\begin{lstlisting}
String path = "/path/to/jar"
admin.disableTable(<table_name>)
hTableDescriptor.setValue("COPROCESSOR$1", path + "|"
	+ RegionObserverExample.class.getCanonicalName() + "|"
	+ Coprocessor.PRIORITY_USER);
admin.enableTable(<table_name>)
\end{lstlisting}

\textbf{0.96 or newer} \newline
\lstset{language=Java}
\begin{lstlisting}
String path = "/path/to/jar"
admin.disableTable(<table_name>)
hTableDescriptor.addCoprocessor(<class_name>.class.getCanonicalName(),
			 path, Coprocessor.PRIORITY_USER, null);
admin.enableTable(<table_name>)
\end{lstlisting}

\subsection{Dynamic Unloading of coprocessor}
Dynamic unloading of coprocessor can also be done in two ways, from shell and from Java API. 

\subsubsection{Using HBase shell}
\begin{enumerate}
	\item disable table
	hbase\textgreater disable '\textless \texttt{table\_name}\textgreater'
	\item alter table, remove coprocessor
	hbase\textgreater alter '\textless \texttt{table\_name}\textgreater', \newline
	METHOD =\textgreater '\texttt{table\_att\_unset}', NAME=\textgreater 'coprocessor\$1' =\textgreater
	\item enable table
	hbase\textgreater enable '\textless \texttt{table\_name}\textgreater'
	 
\end{enumerate}

\subsubsection{Using Java API}
Using Java API, in the newer version we can use removeCoprocessor() method provided by HTableDescriptor class and in the older version, we can use setValue() to unload coprocessor.

\section{Proposed Method}
In this section we will explain about the algorithms we've implemented to incrementally maintain materialized views for 
\begin{enumerate}
	\item Aggregation
	\item Join and Aggregation
	\item Join and Selection
\end{enumerate}

One of the most important feature in our implementation is the introduction of intermediate table. We have introduced intermediate table in order to restrict the scanning of entire base table for a simple get, put or delete operation. Scanning billions of rows for such operations can be expensive in terms of processing power and CPU usage. 

\subsubsection{Creation of Intermediate table}
If there are two base tables, then we merge column families of both tables into the intermediate table. If there is only one base table, then we have the same column family in our intermediate table. The $key$ from base table becomes row key for the intermediate table, $row key$ of base table becomes $column$ $qualifier$ in the intermediate table. So the value for key and row key from base table is now plotted in intermediate table for that particular key. The figure below explains transformation of base table into intermediate table in more detail. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{IntermediateTable}
	\caption{Intermediate Table}
	
\end{figure}

\newpage
\subsection{Aggregation}
In our implementation, we've implemented basic aggregation functions like sum, count, min and max. All these operations are carried out based on a particular key. The base table contains key,value pairs. We construct an intermediate table from the base table. The reason behind constructing intermediate table is to restrict scanning of whole base table for a update/delete trigger for a particular key in base table. We take the unique keys and map them as a row key in intermediate table, and accordingly the values are plotted. Once all the values are plotted in the intermediate table, we then construct view table. The view table contains aggregate functions like Sum, Count, Min and Max for each row Key i.e. for each unique keys of the base table. The figure below explains how we map base table to a view table.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Aggregation}
	\caption{Aggregation}
	\label{sec:aggregation}
	
\end{figure}

\newpage
Once we have base table, intermediate table and view table, and successfully loaded coprocessor on our base table, we are ready to go ahead with our implementation. There are certain scenarios where coprocessor are triggered for an update and delete operations.

\begin{enumerate}
	\item New row is inserted
	\item Existing value of a row is updated
	\item Existing key of a row is update
	\item Existing row is deleted
\end{enumerate}

\subsubsection{New row is inserted}
Whenever a new row is inserted in a base table with key,value pair, the value has to be plotted in the intermediate table accordingly. 


\newpage
\chapter{Discussion and Conclusion}


		% ---------------------------------------------------------------------------
		%
		% Appendix
		%
		% ---------------------------------------------------------------------------
		
		\part*{Appendix}
		\addcontentsline{toc}{part}{Appendix}
		
		\appendix %---------------------------------------
		
		%\input{chapters/oneAppendix}

  \clearemptydoublepage
  
	\bibliography{bibliography/literature}
	
 
\end{document}

