 % The main file for CAMP reports
 % Don't put any content in here. 
 % Don't even include content files by using \input or \inlcude. 
 % Put your content to TEXT.TEX or include it there using \input.
 % Uses:
 %		SETTINGS.TEX	contains the settings for this document
 %		COMMANDS.TEX	contains commands which can be used while writing
 %		INFO.TEX			contains the author, title and so on for the cover
 %		COVER.TEX			formats\documentclass[10pt]{?} the front cover of the document
 %		ABSTRACT.TEX	contains the abstract to be included (if needed)
 %		TEXT.TEX			contains the actual content of the document
 %		BIB.BIB				containt the BibTeX entries for the document
 
 
%% Draft document mode
%% Final document
\documentclass[11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR12mm,DIV13]{scrbook}


%\documentclass[11pt,a4paper,bibtotoc,idxtotoc,headsepline,footsepline,footexclude,BCOR20mm,DIV10]{scrbook}

% KOMA-Optionen:
%  bibtotoc: include bibliography in table of contents
%  idxtotoc: include index in table of contents
%  headsepline: use horizontalline under heading
%  BCOR: binding correcion (Bindungskorrektur) (e.g.: BCOR5mm)
%  DIV: Number of sheet sections (used for layout) (e.g.: DIV12) 


% include title and author information for the cover
\input{components/info}

% include settings
\input{components/settings}

% include commands
\input{components/commands}


%\makeindex
	%% inter line spacing
%\linespread{1.0}

\makeglossary

\begin{document}

	\frontmatter
	
	
	\input{components/cover}
%	\clearemptydoublepage
	
	%\input{components/titlepage}
	
	
%	\input{components/cover_maschmeyer}
	\clearemptydoublepage
	
	\input{components/titlepage}
	
	
	\input{components/disclaimer}
	
	\input{components/acknowledgements}
	
	\input{components/abstract}

	\tableofcontents
  
 % \input{components/outline}

	\mainmatter
	
	



\chapter{Introduction}
\label{chap:introduction}

Whenever we see our friends posting pictures on Facebook or Instagram, we like them or comment on them. Whenever we feel like sharing our thoughts, we either update status on Facebook or just tweet about it. If we need some relevant information, we just google it. The amount of data generated in such a fashion has to be stored somewhere. Companies like Facebook stores 500 TB of data each day\cite{daniel:datastats}, including 2.7 billion likes and 300 million photos. As of 2012, Facebook already has 100 petabytes of photos\cite{daniel:datastats}. Google, on the other hand, processes 3.5 billion requests per day \cite{daniel:datastats}. In the early 2000s, where there were fewer data shared on social media, data were stored in a relational database. Relational databases were designed in such a fashion to store a small amount of data and maintain integrity between them\cite{matt:rdb}. The amount of information we share on social media is expected to grow from 4.4 zettabytes in 2013 to 44 zettabytes in 2020(1 zettabyte is 1 trillion gigabytes)\cite{matt:rdb}. The scaling in RDBMS depends on adding more powerful CPU's and memory, i.e. only the vertical scaling is possible which is rather expensive.  One of the advantages of the big data storage system is that it can be scaled horizontally and is also useful for storing unstructured or semi-structured data. 

HBase is an open source sortedMap Datastore from Apache Software Foundation which is used as a database to store huge volume of data. HBase supports horizontal scalability, i.e. parts of a table can be put on several machines. This way a table is broken down into multiple pieces, thus making computation fast. But when we are talking about petabytes of data, scanning each part of the table for a single user query is still considered to be expensive regarding processing time. There are several techniques to reduce this effort, but we will be talking about $Materialized$ $Views$ approach. 



%In Chapter \ref{chap:background}, we analyse the fundamentals of views and view maintenance. In Chapter \ref{chap:relatedwork}, we present research, that is related to this thesis. In Chapter \ref{chap:analysis}, we define the requirements of the View Maintenance System and discuss possible alternatives.  In Chapter \ref{chap:architecture}, we align the architecture of the system and define its functionalities. In Chapter \ref{chap:viewconsistency}, we discuss threats to consistency and apply consistency techniques. In Chapter \ref{chap:loadbalancing}, we show how load balancing can be accomplished in the View Maintenance System. In Chapter \ref{chap:failuredetection}, we take counter measures to component failure. In Chapter \ref{chap:implementation}, we show challenges of the implementation. Finally, we evaluate and interpret the behaviour of the system in Chapter \ref{chap:evaluation}.\\%


\chapter{Background}
\label{chap:background}

In this chapter we will first discuss about the fundamentals of $Materialized$ $Views$ and $View$ $Types$. We will further explain about the technologies used widely in today's Distributed Storage Databases. 

\section{Views}
In a relational database management system, a $View$ is defied as result set of a query. View can be subset of a table or joins from multiple tables. Views in relational database systems are generally created for frequently accessed queries involving multiple joins to reduce cost of the operation. Views are nothing but a $SELECT$ statements to fetch desired result sets and are given certain name and saved in database. Views can also hide a complexity of a query. In a large dataset, when a computation is required to fetch data from several tables involving complex business logic, all the complex business logic can be moved to a $view$, and then just use $SELECT$ statement to get data from that view thus hiding the complexity of a query. Views also provide a layer of security mechanism to our database table. We can create a view without the columns containing confidential information, and restrict access to the base table. We can then provide access to the view and carry out desired operation using that view. 

In relational database systems, Views are widely used. However, there are also certain disadvantages of $Views$. In a scenario where base table is deleted, the view of that table becomes inactive. In $MySQL$ database, for every client request, a view is recalculated. This might not be a problem for small applications containing few hundred rows or columns, but re-calculating views for every client request in large dataset can be a bottleneck for performance optimization. To overcome this bottleneck, a new approach called $Materialized View$ is used.


\section{Materialized Views}

Materialized view is defined as the database object that stores the result of a query in a table or a disk. Materialized views are widely used for gaining performance advantage, i.e. to speed up query processing time over large datasets. The need for Materialized view addresses the problem of having to query large datasets that often needs joins and aggregations between multiple tables. These kind of queries are very expensive regarding execution time and processing power. Materialized views speed up the query processing time by pre-computing joins and aggregations before execution and stores these results in a table or disk\cite{materializedview:oracle}. 

\section{View Maintenance}
Once the Materialized views are created, our query is redirected to Materialized View table rather than base table. Whenever there is an update in the base table, the Materialized View table also has to be updated accordingly. One of the solutions would be recomputing the entire Materialized View from the scratch or using the heuristic of inertia\cite{maintenance:materializedviews} approach i.e. incremental maintenance with respect to the base table.

\section{Incremental Maintenance of Materialized View}
"A view V is considered consistent with the database DB if the evaluation of the view specification S over the database yields the view instance (V = S(DB)). Therefore, when the database DB is updated to DB0 , we need to update the view V to V0 = S(DB0) in order to preserve its consistency"\cite{incrementalmaintenance:materializedviews}. 

Recomputing Materialized view from scratch every time there is an update on base table is expensive. The other approach is to update the part of Materialized view table with respect to the update in Base Table. Our target is to maintain consistency between Materialized views and base table whenever there is an update on the base table.


\subsection{Aggregation}
In Aggregation view type, the data from the base table is merged on the basis of a particular key. In our implementation, we've implemented basic aggregation functions like sum, count, min and max. All these operations are carried out based on a particular key. So a unique key has sum, count, min and max operations. Whenever an update is triggered to update value for a particular key in the base table, in this case, count remains same and sum, min and max has to be recalculated. If a delete is triggered for a particular key in the base table, each of the aggregation functions has to be recalculated. 


\subsection{Join and Aggregation}
In Join and Aggregation case, we have at least two base tables. Joins being one of the complex structure itself, incremental view maintenance implementation involves a lot of complex cases. Here, to reduce complexity, we join two base tables on the basis of $key$ to form a new intermediate table. We group all the values of both base tables based on their keys. This way, for any update or delete trigger, the complexity of scanning whole base table is reduced to a single row. In our intermediate table, each of the base table is merged to a column family, join is applied and then result is stored in the view table. 

In the intermediate table, the unique keys from both the base tables act as the row key, both column families from base table are merged in the intermediate table. Now for a particular row key, the values are selected from base table and plotted in the intermediate table. Now join is applied between both column families of a particular row key, and sum of the join is inserted in the view table.  


\subsection{Join and Selection}
Join and Selection case is similar to the Join and Aggregation case, the only difference is instead of applying aggregation function, the join is applied for a particular row key and value is selected and inserted into the view table.

\newpage
\section{HBase}
\label{sec:hbase}

Before the evolution of HBase, Relational database systems were used particularly for storing and processing of data. Relational database systems have been used widely over a decade and are considered to be successful. In a relational database, multiple tables are used to store different types of data, this segregation of data gives more clear and systematic view of the data\cite{relational:dbs}. However, one of the biggest drawbacks in Relational database is the difficulty of scaling horizontally. The major disadvantage of relational database design is the performance if the number of tables between which the relationships has to be defined is large, i.e. more operation power needed for computation\cite{relational:dbs}. 

HBase is an open source sortedMap Datastore from Apache Software Foundation. HBase is modelled after Google's BigTable framework. It is a Hadoop database that is used for storing and retrieving data with random access. HBase architecture is designed to run on a cluster of computers rather than a single machine \cite{coprocessor:detail}. HBase aims to scale horizontally by adding more machines to the cluster. HBase runs on top of HDFS(Hadoop Distribution File System) that provides the functionality alike of Google's Big Table and provides a fault-tolerant way of storing a large volume of semi-structured and unstructured data\cite{bigdata:analysis}.

HBase is built on top of Hadoop and Zookeeper\cite{coprocessor:detail}. Both Hadoop and Zookeeper are open source projects from Apache Software Foundation. Apache Hadoop is an open source framework that facilitates storing and processing large dataset in a distributed fashion. Zookeeper, which was developed under Apache software foundations, as a sub-project of Hadoop, is an open source distributed configuration service for large distributes applications. A basic table structure of HBase consists of Row Key, which is similar to the primary key in a relational database table, Column Family and Column Qualifier. The figure below describes a HBase table and it's mapping to the relational database table.


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{HBase}
	\caption{HBase}
	\label{fig:HBase}
\end{figure}

\newpage
\subsection{HBase Architecture}
\label{HBase Architecture}
HBase architecture consists of three major components and three sub components. The major components are Master, Region server and zookeeper. The three sub components are Write-Ahead-Log(WAL), HFile and Memstore\cite{hbase:insights}. HBase architecture is based on Master-Slave architecture, where the Master is known as HMaster, is the master node and Region Servers are the slave nodes. Whenever the write request is sent, HMaster receives the request and forwards it to the respective Region Server\cite{hbase:insights}. 

\subsubsection{HMaster}
\label{hmaster}
HBase Master is mainly responsible for region assignments within the region servers and DDL operations like creating and deleting tables\cite{hbase:architecture}. Apart from these roles, HMaster is also responsible for assigning the regions and re-assigning of the regions for recovery or load balancing\cite{hbase:architecture}. HMaster also monitors all the instances of Region Servers in the cluster\cite{hbase:architecture} and mainly provides administrative operations.

\subsubsection{Region Servers}
\label{region serves}
Region servers are systems within HBase that acts like a data node\cite{hbase:insights}. When a HMaster receives a write request, it forwards the request to the Region Server. Region server can have multiple regions within it, and it directs the request to the specific region. Region servers are mainly responsible for handling data related operations and communication. Region servers handle the read/write request for all the regions within it. 
A Region Server runs on data node and it has four sub components as described below\cite{hbase:architecture}
\begin{description}
	\item[$\bullet$]  Write Ahead Log(WAL): Write Ahead Log is basically a log file. Region server adds each request to WAL first before sending that request to the appropriate region. It is mainly used for recovery in case of failure\cite{hbase:insights}. If the request is not written in the WAL file, there is a possibility of data loss in case of Region Server failure.
\end{description}

\begin{description}
	\item[$\bullet$]  BlockCache: BlockCache is the read cache that is used to store frequently read data\cite{hbase:insights}. When the cache is full, last read data is removed from the cache.
\end{description}

\begin{description}
	\item[$\bullet$] MemStore: MemStore is the write cache. All the new data that has not been written to the disk are stored in MemStore. It is mainly responsible for keeping tracks of all the logs for read and write operations to be performed for a specific Region Server\cite{hbase:architecture}. Each column family in a region has one MemStore\cite{hbase:insights}.
\end{description}

\begin{description}
	\item[$\bullet$] HFile: In HBase, column family is a collection of multiple HFiles. HFiles are used to store rows as keyValue pairs and are immutable and sorted\cite{hbase:insights}.
\end{description}

\subsubsection{Zookeeper}
\label{zookeeper}
Zookeeper an open source project under Apache Software Foundations, is a distributed software system that provides a infrastructure for synchronization across the clusters. It provides coordination between distributed processes across the cluster so that client receives consistent data. The architecture of Zookeeper is based on client-server model. The client acts as a node that make use of the service and server acts as a node that provides the service\cite{hbase:zookeeper}. Many Zookeeper servers can be collected together, that is known as $Zookeeper$ $ensemble$\cite{hbase:zookeeper}. Each server node of the zookeeper at a given time can handle large number of client connections. It is essential to know if the connection is alive, so the client node sends a ping request to the server it is connected to make sure it is connected and alive\cite{hbase:zookeeper}. The server, after receiving ping request, sends an acknowledgement to indicate that server is alive. If the client doesnot receive acknowledgement within a given specific time, then the client connects to another zookeeper server within a $Zookeeper$ $ensemble$ and the client session is transferred to the new zookeeper server\cite{hbase:zookeeper}.
\newline
HBase has a tight integration with Zookeeper. HBase uses Zookeeper as a distributed coordination service to facilitate synchronization between the servers in a cluster. HBase also uses Zookeeper to keep track of state of the servers, which servers are alive and available\cite{hbase:architecture}. Whenever a HBase instance is started, it automatically starts Zookeeper instance, as Zookeeper comes integrated with HBase\cite{hbase:insights}. Zookeeper is used to keep tracks of the number of regions servers available, and the data hold by each region servers. 
\newline

The figure below explains the HBase Architecture and its components.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{HBase_Architecture}
	\caption{HBase Architecture}
	\label{fig:hbasetable}
\end{figure}

\newpage
\section{Hadoop Distributed File System}
\label{Hadoop Distributed File System}
The Hadoop Distributed File System (HDFS) is an open source distributed file system developed for the Hadoop framework. HDFS is designed to store very large data sets and run on a commodity hardware. In HDFS, each files are divided into blocks of fixed size and are stored across multiple machines\cite{HDFS:architecture}. HDFS is also based on client-server architecture and each HDFS cluster consists of a single NameNode, also called as Master Node and multiple DataNodes, known as Slave nodes. All the metadata are stored in NameNode wheres the application data resides in DataNodes\cite{hadoop:architecture}.

\subsection{NameNode}
\label{NameNode}
NameNode is the master node of HDFS file system. NameNode is the centerpiece in the HDFS architecture and is responsible for keeping the directory tree of all the files in a system\cite{HDFS:namenodeanddatanode}. Namenode does not store any data of the files, it stores 
only the metadata like namespace information and block information of HDFS\cite{HDFS:namenodeanddatanode}, and the data are stored in Data nodes. NameNode maps files into the set of blocks, and maps those blocks to a data nodes and directs data nodes to execute I/O operations\cite{HDFS:namenodeanddatanode}. For example, when a client wants to locate a particular file, Namenode intercepts the request from the client and returns response by retrieving all the possible data nodes where the file resides. Since there is only one NameNode in the HDFS cluster, it is subjected to a single point of failure in HDFS Cluster. If a Namenode is down, all the running processes will terminate as a result of which the entire HDFS cluster goes offline\cite{HDFS:namenodeanddatanode}.  

\subsection{SecondaryNameNode}
\label{SecondaryNameNode}
As the name suggests, it is assumed that SecondaryNameNode is used as a backup node in case of single point of failure. From subsection \ref{NameNode}, we know that NameNode stores meta information like namespace and block information. All these informations are stored in main memory and also in the physical disc for persistence storage\cite{HDFS:secondarynamenode}. Whenever a NameNode is started, the snapshot of the file system is stored in fsimage file and logs of the changes made after NameNode is started is written in Edit logs. There might be an issue when edit logs become very large and hard to manage. So SecondaryNameNode is used as a checkpoint in the HDFS. It fetches the edit logs from the namenode in reqular interval and updates fsimage with edit logs.The recent fsimage is copied back to the NameNode\cite{HDFS:secondarynamenode}. Since SecondaryNameNode cannot process the metadata to the disc\cite{HDFS:architecture}, it can not be used as a substitution to the NameNode.

\subsection{DataNode}
\label{DataNode}
DataNodes are the slave nodes in the HDFS file system. There can be one or many data nodes in a HDFS cluster. The data nodes are responsible for storing the files in a HDFS cluster. When the DataNode is started, it sends information about all the files and blocks stored in that node to the NameNode\cite{HDFS:datanode}. DataNode, likewise NameNode, is also expected to fail at some point. But this doesnot let the HDFS cluster to go offline. In such scenario, NameNode will replicate the blocks and files managed by failed DataNode\cite{HDFS:namenodeanddatanode}. 


\section{Coprocessor}
\label{sec:coprocessor}

HBase Coprocessor framework provides a library to run user code in the HBase Region Server. The advantage of this framework is that it decreases the communication overhead of transferring the data from HBase region server to the client, thus improving the performance by allowing the real computation to happen in the HBase region server\cite{coprocessor:coprocessor}. There are two types of coprocessor, Observer coprocessor  which acts more like relational database triggers and Endpoint coprocessor that resembles stored procedures of RDBMS\cite{coprocessor:types} 


\subsection{Observer coprocessor}
\label{sec:observercoprocessor}
Observer coprocessor as stated earlier, is more like database triggers that executes our code when certain events occur. In the figure below, we first try to explain a simple life cycle of put() operation as an example\cite{coprocessor:detail}. Observer coprocessor resides between the client and the HMaster. Observer coprocessor can be triggered after every get(), put() or delete() command. The CoprocessorHost class is responsible for observer registration and execution\cite{coprocessor:detail}.  During the life cycle of events, Observer coprocessor allows us to hook triggers in two stages. The first one is before the occurrence of the event and the other is after the completion of the event. For example, if we want to perform some computations before the occurrence of put event, we can use prePut() method to perform our custom computation. Then the life cycle of put event starts and after the life cycle of put event is completed, we can use postPut() method to perform custom computation. In the figure below, we try to explain the lifecycle of observer coprocessor when a put event is fired\cite{coprocessor:detail}.
\newline
There are four types of Observer Interfaces provided as of HBase version 1.1.3\cite{hbase:essential}.

\begin{enumerate}
    
\item RegionObserver: RegionObserver runs on all the Region of a HBase table. RegionObserver provides hook for data manipulation for events like put(), get() add delete() events. All the data manipulations are done with pre-hook and post hook\cite{hbase:essential} such as pre and post observers. For instance, preGetOp() and postGetOp() provides hook for manipulating get request. 

\item RegionServerObserver: Likewise in RegionObserver, RegionServerObserver provides a hook for data manipulation for events like merge, commits and rollback. All the data manipulation are done with pre-hook and post hook such as preMerge() and postMerge(). 

\item WALObserver: WALObserver interface provides a hook for Write-Ahead-Log(WAL)\cite{hbase:essential} related operations. This interface provides only preWALWrite() which is triggered before WALEdit is written to Write-Ahead-Log and postWALWrite() which is triggered after WALEdit is written to a Write-Ahead-Log.

\item MasterObserver: MasterObserver Interface provides a hook for data manipulation for DDL events such as table creation, table deletion or table modification\cite{cloudera:instandupg}. For instance, if the secondary indexes need to be deleted when primary table is deleted, we can use postDeleteTable(). The MasterObserver runs on the master node.

\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{PutRequestLifeCycle}
    \caption{Life cycle of put request}
    
\end{figure}

In the figure below, We can see the life-cycle of a put request with observer coprocessor implemented.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{PutRequestWithObserverCoprocessor}
    \caption{Life cycle of put request with observer coprocessor}
    
\end{figure}

\newpage
\subsection{Endpoint coprocessor}

Endpoint coprocessor is similar to the Stored Procedures in RDBMS. This type of coprocessor is more useful in the scenario where the computation is needed for the whole table and are not provided by observer coprocessor\cite{hbase:coprocessors}. Invoking the endpoint coprocessor is similar to invoking any other commands in HBase from the client\textquotesingle s point of view but the result is based on the code that defines the coprocessor\cite{coprocessor:detail}. The figure below explains the Aggregation example\cite{coprocessor:detail}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{EndPointCoprocessor}
    \caption{EndPoint Coprocessor}
    
\end{figure}

When a request is invoked from a client, an instance of Batch.call() encapsulates the request invocation and the request is forwarded to coprocessorExec() method of HTableInterface. Then the coprocessorExec() handles the request invocation and distributes the request to all the Regions of the RegionServer. Assuming that no interruptions occurs and all the requests are completed, the results is then returned to client and aggregated\cite{coprocessor:detail}.

\newpage
\chapter{Related Work}
In this chapter, we will discuss the existing research that have been made to maintain consistency between base table and view table.

\section{Foundations}
\label{Foundations}
In the early 90's when relational database systems were popular and widely used, several research were conducted to optimize the query processing time. The idea of using materialized view when-ever possible to evaluate a query for the benefit of improved query processing was proposed more than a decade ago\cite{maintenance:optimizingqueries}. Several methods have been proposed for incremental view maintenance in the past\cite{blakeley:efficiently, gupta:maintaining, agrawal:efficient}.

In\cite{incremental:materializedviews}, the researchers have investigated the problem of incremental maintenance of a materialized view. The researchers in paper\cite{incremental:materializedviews} have proposed an auxiliary relations to reduce the cost of view maintenance. They proposed a view that can be represented by an operator tree\cite{database:concepts} where leaf nodes represented database relations and non-leaf nodes represented relational algebraic operations. An auxiliary relation was maintained for each node, and the key of auxiliary relations is a foreign key that matched the primary key of each relation, thus maintaining referential integrity between auxiliary relations and base relations\cite{database:concepts}. These auxiliary relations are also changed in response to the base relations changes.

In paper \cite{incrementalmaintenance:materializedviews}, the researchers have demonstrated an algorithm for incremental view maintenance based on graph-based data model and query language Lorel developed at Stanford. Their algorithm produces a set of queries that computes the changes to be made to the view table based on the changes to the base table. Researchers proposed a view specification extension to Lorel query language\cite{incrementalmaintenance:materializedviews} that introduced two objects in the view model: $select-from-where$ and $with$. The $select-from-where$ model specified the primary objects imported to the view and the later one $with$ model specified paths from primary objects to the adjunct objects\cite{incrementalmaintenance:materializedviews}. Their algorithm generates a set of maintenance statements for a given view and a database object, evaluates the updates on the database to generate new set of view updates and finally installs the updates in the view\cite{incrementalmaintenance:materializedviews}.

In paper \cite{maintenance:materializedviews}, the researchers have classified four dimensions along which the view maintenance problems has to be studied. 
\begin{itemize}
	\item Information Dimension: This dimension deals with the amount of information available for view maintenance. Some of the prior information regarding integrity constraints and keys, access to materialized views has to be known before developing an algorithm for incremental maintenance.
	
	\item Modification Dimension: This dimension deals with problem statements related to modification of a system. Some prior knowledge has to be acquired such as what modifications can be handled by a view maintenance algorithm, how update tuples are handled, are they handled directly or are they modeled as deletion followed by insertion tuples.
	
	\item Language Dimension: This dimension addresses problem related with select-project-join query, i.e. does view consists of entire SQL or subset of SQL. It also defines problem statements whether SQL statement can use aggregation function, recursion function or closure. 
	
	\item Instance Dimension: This dimension addresses problems related to instance of database such as if view maintenance algorithm works for all the instances of the database for just for some particular instances. 

\end{itemize}

In paper \cite{maintenance:optimizingqueries}, researchers found that blind applications using materialized views resulted in much worse results than application not using materialized views. The research found out that using materialized views to optimize query performance depends on the query and statistical properties of the database\cite{maintenance:optimizingqueries}. The statistical properties of the database are time-varying and also most of the times, queries are generated using tools, $cost-based$ decision has to be taken whether to use or not to use materialized views to answer a given query in the database\cite{maintenance:optimizingqueries}. There might also be cases where more than one materialized view can be relevant for a given query, so in this case, incorrect alternatives has to be avoided to gain performance advantage. Researchers in paper \cite{maintenance:optimizingqueries} have proposed an algorithm for optimizing materialized views in three steps. In the first step, the query is translated into canonical unfolded form, i.e. system that supports views. In the second step, they identify possible ways to generate one or more materialized views for a given query. In the third step, they use efficient join enumeration algorithm to predict the cost of each alternative formulations and the path with least cost is selected\cite{maintenance:optimizingqueries}.

\newpage
\chapter{Implementation}
In this section, we will first discuss the prerequisite of implementation and then the proposed method for our research.

\section{Prerequisite}
Before we begin with our implementation of the coprocessor, there are few steps to load coprocessor into our HBase table. The coprocessor can be loaded to the base tables in two ways: statically and dynamically\cite{loading:coprocessors}. 

\subsection{Static Loading of coprocessor}
We have to define coprocessor properties in a $hbase$-$site.xml$ file inside a \textless property\textgreater \space element followed by \textless name\textgreater \space and a \textless value\textgreater \space sub element. The \textless name\textgreater \space sub element should have one of the followings\cite{hbase:site}:

\begin{enumerate}
    \item hbase.coprocessor.region.classes for RegionObservers and Endpoints coprocessor
    \item hbase.coprocessor.wal.classes for WALObservers
    \item hbase.coprocessor.master.classes for MasterObservers
    
\end{enumerate}

The \textless value\textgreater \space sub-element should contain the full path of the coprocessor implementation class. A typical example for static loading of coprocessor looks as,\newline \newline \textless property\textgreater \newline
\textless name\textgreater hbase.coprocessor.region.classes\textless /name\textgreater \newline
\textless value\textgreater \texttt{org.apache.hbase.HBase\_coprocessor.HBaseCoprocessor}\textless /value\textgreater \newline
\textless /property\textgreater \newline

 If we have multiple classes, then the path in \textless value\textgreater \space sub-element should be comma separated. In this setup, the framework will attempt to load all the configured classes, so we have to create a jar with dependencies, for all the classes and place the location of the jar to HBase classpath. For that, we have to export /path/to/jar in $hbase$-$env.sh$ file. A typical example for exporting classpath is given below, \newline \newline
 export \texttt{HBASE\_CLASSPATH}='/path/to/jar' \newline
 
Now if HBase is restarted without any errors, we have managed to load system coprocessor successfully. 

\subsection{Static Unloading of coprocesssor}
\begin{enumerate}
    \item Delete entry from $hbase$-$site.xml$
    \item Delete entry for $hbase$-$env.sh$
    \item Restart HBase
\end{enumerate}

\subsection{Dynamic Loading of coprocessor}
In this approach, rather than loading coprocessor to all the tables in a Region, the coprocessor is loaded to specific tables of the region. There are two implementations of loading coprocessor dynamically, from HBase shell or using Java API\cite{hbase:site}.

\subsubsection{Using HBase shell}
\begin{enumerate}
	\item disable table \newline
			\space \space hbase\textgreater disable '\textless \texttt{table\_name}\textgreater'
	\item load coprocessor using the following command \newline
	alter '\textless \texttt{table\_name}\textgreater', \newline 
	METHOD =\textgreater '\textless \texttt{table\_att}\textgreater', 'coprocessor' =\textgreater '/file/to/path\text{\textbar}\newline/source/path/to/impementation/class\text{\textbar}1001\text{\textbar}'
	\newline \newline
	A typical example looks like,\newline \newline
	alter 'BaseTableA', METHOD =\textgreater '\texttt{table\_att}', 'coprocessor' =\textgreater 'file:///home/saroj-gautam/Documents/HBase-coprocessor-0.0.1-SNAPSHOT-jar-with-dependencies.jar\text{\textbar}\texttt \newline {org.apache.hbase.HBase\_coprocessor.HBaseCoprocessor}\text{\textbar}1001\text{\textbar}'
	
	\item enable table \newline
	See if coprocessor is loaded successfully. We can see it by seeing the table properties. \newline 
	hbase\textgreater describe '\textless \texttt{table\_name}\textgreater \space should list the coprocessor under \texttt{TABLE\_ATTRIBUTES.} \newline

\end{enumerate}

In the above scenario, the coprocessor tries to read class information from \texttt{table\_att} property. There are certain arguments separated by pipe (\text{\textbar}). The first argument in the value is the file path to the jar file that contains the implementation class. The second argument contains the full classname of the implemented coprocessor. The last argument represents the execution sequence of registered observers. If this field is left blank, the framework will itself assign a default priority value\cite{hbase:site}.

\subsubsection{Using Java API}
Prior to HBase version 0.96, the coprocessors were loaded in a different way. After HBase version 0.96 and newer, HTableDescriptor class provides addCoprocessor() method that helps to load coprocessor in an easier way. A code snippet\cite{loading:coprocessors} below will give us a basic insight of how coprocessor is loaded dynamically from Java API in older versions and newer versions of HBase.
\newline \newline
\textbf{Older than 0.96} \newline
\lstset{language=Java}
\begin{lstlisting}
String path = "/path/to/jar"
admin.disableTable(<table_name>)
hTableDescriptor.setValue("COPROCESSOR$1", path + "|"
    + RegionObserverExample.class.getCanonicalName() + "|"
    + Coprocessor.PRIORITY_USER);
admin.enableTable(<table_name>)
\end{lstlisting}

\textbf{0.96 or newer} \newline
\lstset{language=Java}
\begin{lstlisting}
String path = "/path/to/jar"
admin.disableTable(<table_name>)
hTableDescriptor.addCoprocessor(<class_name>.class.getCanonicalName(),
             path, Coprocessor.PRIORITY_USER, null);
admin.enableTable(<table_name>)
\end{lstlisting}

\subsection{Dynamic Unloading of coprocessor}
Dynamic unloading of coprocessor can also be done in two ways, from shell and from Java API. 

\subsubsection{Using HBase shell}
\begin{enumerate}
    \item disable table
    hbase\textgreater disable '\textless \texttt{table\_name}\textgreater'
    \item alter table, remove coprocessor
    hbase\textgreater alter '\textless \texttt{table\_name}\textgreater', \newline
    METHOD =\textgreater '\texttt{table\_att\_unset}', NAME=\textgreater 'coprocessor\$1' =\textgreater
    \item enable table
    hbase\textgreater enable '\textless \texttt{table\_name}\textgreater'
     
\end{enumerate}

\subsubsection{Using Java API}
Using Java API, in the newer version we can use removeCoprocessor() method provided by HTableDescriptor class and in the older version, we can use setValue() to unload coprocessor.

\section{Proposed Method}
In this section, we will explain about the algorithms we've implemented to maintain incrementally materialized views for 
\begin{enumerate}
    \item Aggregation
    \item Join and Aggregation
    \item Join and Selection
\end{enumerate}

One of the most important features in our implementation is the introduction of intermediate table. We have introduced intermediate table in order to restrict the scanning of the entire base table for a simple get, put or delete operation. Scanning billions of rows for such operations can be expensive in terms of processing power and CPU usage. 

\subsubsection{Creation of Intermediate table}
\label{subsec:intermediatetable}

If there are two base tables, then we merge column families of both tables into the intermediate table. If there is only one base table, then we have the same column family in our intermediate table. The $key$ from base table becomes row key for the intermediate table, $row key$ of the base table becomes $column$ $qualifier$ in the intermediate table. So the value for key and row key from base table is now plotted in intermediate table for that particular key. So whenever there is a CRUD operation for a particular key, we can scan row for the particular key instead of scanning the whole table. The figure below explains the transformation of base table into an intermediate table in more detail. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{IntermediateTable}
    \caption{Intermediate Table}
    
\end{figure}

\newpage
\subsection{Aggregation}
\label{sec:aggregationImpl}

In our implementation, we've implemented basic aggregation functions like sum, count, min and max. All these operations are carried out based on a particular key. The base table contains key,value pairs. We construct an intermediate table from the base table. The reason behind constructing intermediate table is to restrict scanning of the whole base table for an update/delete trigger for a particular key in a base table. We take the unique keys and map them as a row key in the intermediate table, and accordingly the values are plotted. Once all the values are plotted in the intermediate table, we then construct view table. The view table contains aggregate functions like Sum, Count, Min and Max for each row Key i.e. for each unique keys of the base table. 

%The figure below explains how we map base table to a view table.

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{Aggregation}
%    \caption{Aggregation}
%    \label{sec:aggregation}
    
%\end{figure}
%\newpage

Once we have a base table, intermediate table and view table, and successfully loaded coprocessor on our base table, we are ready to go ahead with our implementation. There are certain scenarios where coprocessor is triggered for an update and delete operations.

\begin{enumerate}
    \item New row is inserted
    \item Existing value of a row is updated
    \item Existing key of a row is updated
    \item Existing row is deleted
\end{enumerate}

\subsubsection{New row is inserted}
Whenever a new row is inserted in a base table with (key,value) pair, the (key,value) pair has to be inserted in the base table and we have to plot the new (key,value) pair in the intermediate table and also view table has to be updated accordingly. Using prePut() and postPut() triggers from observer coprocessor, we perform all the required operations.

As we have already discussed put() request life cycle in \ref{sec:observercoprocessor}, before the (key,value) is inserted, we catch the request using prePut() method provided by the observer coprocessor. In the prePut() method, we verify the inputs and check if new row is inserted or existing row is updated. After we verify that new row is being inserted, we let the request to insert new (key,value) pairs to be inserted into the base table. After new (key,value) pair is inserted into the base table, we again catch the request in postPut() method. In postPut() method, we plot the (key,value) pair in the intermediate table and then update aggregation functions in our view table. The figure below explains the scenario when a new row is inserted. The left side tables are the default tables and right side tables explain the behavior when a new row is inserted. The text displayed in red mark the changes that are happening on base table, intermediate table and view table. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{NewRowInsertAggr}
    \caption{New row insert}
    \label{sec:newrowinsertagggregation}
    
\end{figure}

\newpage
\subsubsection{Existing value of a row is updated}
Whenever an existing value of a key is updated, the base table is updated accordingly. Before the base table is updated, we catch the request via prePut() method of observer coprocessor. In the prePut() method, we get the row key for which the value is going to be updated and also we verify if the value is being updated or the key is being updated. After we verify that value is updated, then we release the request and the value is updated in the base table. After the insertion, we catch the request via postPut() method of observer coprocessor, and then plot the updated value in our intermediate table for particular row key. Since we already have  the row key, we only need to can that particular row, instead of scanning the whole table. This saves a lot of execution time and processing power. Once we plot updated value in the intermediate table, we then calculate aggregation functions for that particular row key and then update our view table accordingly.
The figure below explains the process in more detail. The updated value is marked in red on the right table, and also from the figure, we can see that we only iterate over a particular row key instead of scanning the whole base table and view table.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{ValueUpdateAggr}
    \caption{Update value for a existing row key}
    \label{sec:updatevalueforexistingkey}
    
\end{figure}  

\newpage
\subsubsection{Existing key of a row is updated}
\label{subsubsec:keyupdate}
Whenever there is a trigger for $key$ of the particular row key to be updated, we first catch the request via prePut() method. In this scenario, we first have to find out the $key$ to be updated and delete the plotting from intermediate table. In the prePut() method, we find the (key,value) pair for a $key$ to be updated and store it somewhere in memory. Then we release the request and the $key$ is updated in the base table. In this case, now we have $old$ $key$ and the $new$ $key$. 

In the postPut() method, first we find the column to be deleted from the intermediate table. Then we delete that particular column and update our view table accordingly. After the process is complete without any interruption, the process is similar as of inserting new (key,value) pair. We plot the $new$ $key$ and $value$ in our intermediate table and update the view table accordingly. This is the most complex scenario because it might affect more than one row in our view table. In the figure below, the old key $A$ is updated to new key $B$. In the intermediate table, the plotting for old key $A$ is deleted and aggregation functions for old key $A$ are also updated in the view table. After the process is completed, new values for updated key $B$ is plotted in the intermediate table and then the view table for row key $B$ is also updated accordingly.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{KeyUpdateAggr}
    \caption{Update Key for a existing row key}
    \label{sec:updateKeyforexistingrow}
    
\end{figure} 

\newpage
\subsubsection{Existing row is deleted}
\label{subsubsec:rowdelete}
When an existing row is deleted in the base table, in the postPut() method of observer coprocessor, we delete the plotting for that particular $key$. In this case, there are two scenarios. If the $key$ to be deleted has more than one values in the intermediate table, then we delete the particular plotting in the intermediate table and update aggregation functions for that $key$ in the view table. If the $Key$ in the intermediate table only has a single plotting, then we delete that plotting from an intermediate table and then also delete the entry for that $key$ from the view table.

In the figure below, we have a delete() call for row key $5$. The $key$ for row key $5$ is $C$. Now we delete the row key $5$ from the base table. After that, we delete the plotting for key $C$ in our intermediate table. Since the key $C$ has only one plotting, we delete entry for row key $C$ from the view table instead of recomputing aggregation functions for row key $C$. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{DeleteExistingRowAggr}
    \caption{Delete an existing row}
    \label{sec:deleteexistingrow}
    
\end{figure} 

\newpage
\subsection{Join and Aggregation}
\label{subsec:joinandaggregation}
In this subsection, we have implemented Join and Aggregation functionalities. For this, we have joined two base tables and in the view table, we have the sum of the values for a same $key$. We also have an intermediate table where both the base tables are merged by a particular $key$ and values are plotted in the intermediate table base on that $key$. As described in \ref{sec:aggregationImpl}, we have implemented sum for same $key$ in both the base tables. 

%The figure below explains how we map base table to a view table.

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{JoinAndAggregation}
%    \caption{Join and Aggregation}
%    \label{sec:joinandaggregation}
    
%\end{figure}

%\newpage
There are certain scenarios where coprocessor is triggered for an update and delete operations.

\begin{enumerate}
    \item New row is inserted
    \item Existing value of a row is updated
    \item Existing key of a row is updated
    \item Existing row is deleted
\end{enumerate}

\subsubsection{New row is inserted}
Whenever a new row is inserted in a base table with (key,value) pair, the (key,value) pair has to be inserted in the base table and we have to plot the new (key,value) pair in the intermediate table and also view table has to be updated accordingly. Here, we create an intermediate table by merging two base tables and plotting values accordingly. The algorithm for plotting values has already been discussed in sub section \ref{subsec:intermediatetable}. In the view table, we implement sum function for same $keys$ in both base tables.
The figure below describes the scenario in more detail. The updated table on the right has new values plotted in red.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{NewRowInsertJoinAggr}
    \caption{New row is inserted for Join and Aggregation}
    \label{sec:insertnewrowjoinandaggr}
    
\end{figure}

\newpage

\subsubsection{Existing value of a row is updated}
When a value of a $key$ is updated in the base table, the consistency should also be maintained in the view table. So when the base table is updated, the request is triggered by the postPut() method and intermediate table and view tables are updated accordingly.

\subsubsection{Existing key of a row is updated}
As described in \ref{subsubsec:keyupdate}, when $key$ is modified in the base table, we first have to delete an entry from the intermediate table and view table if the row exists for that particular $key$. Once delete operation is completed without interruption, then we insert the updated key in the intermediate table and view table.

\subsubsection{Existing row is deleted}
The implementation for this scenario is same as described in subsection \ref{subsubsec:rowdelete}. The entry is first deleted from the base table, and if an entry exists in the intermediate table and view table, the entries are deleted from those table accordingly.

\subsection{Join and Selection}
\label{subsec:joinandselection}
This approach is similar to approach described in subsection \ref{subsec:joinandaggregation}. We do not use any aggregation functions but instead just select the values and put them in the view table. As already described in \ref{subsec:joinandaggregation}, there are also four possible scenarios where we update view table incrementally.

\newpage
\chapter{Evaluation}
In this chapter we perform different kinds of experiments on both Pseudo Distributed mode (single node cluster) and Fully Distributed mode (multi node cluster). We will further discuss about every types of experiments we performed and the dataset we used. We will then present the result of our experiments.

\section{Experiment Setup (Pseudo Distributed Mode)}
In pseudo distributed mode, we have performed four different kinds of experiments for each of the three scenarios. First experiment $View$ $Re$-$computation$ $vs$ $Maintenance$ is performed on three different datasets while rest of the experiments are performed on fixed dataset.

\subsection{Deployment}
We performed our experiment on a single node cluster (Ubuntu 16.04 LTS, Intel Core i5-3230M CPU @ 2.60GHz, 3.9GB RAM, 23GB HD). We installed hadoop in pseudo distributed mode. The services like JobTracker, TaskTracker, Namenode and Datanode runs as a separate Java process in a single cluster. We installed hadoop version 2.6.4 and hbase version 1.1.5 for our experiments.

\subsection{Table Configuration}
\label{Table Configuration standalone}
For $Aggregation$, we first create one empty base table, an intermediate table and a view table. We first insert records in the base table. After that, we read data from base table and write into intermediate table as explained in section \ref{sec:aggregationImpl}. Once write in the intermediate table is completed, we perform different aggregation functions like $Sum$, $Max$, $Min$ and $Count$ based on the $key$ of intermediate table and write the result in view table.
\linebreak
\linebreak
For $Join$ \& $Aggregation$ and $Join$ \& $Selection$, we create two base tables as it involves k-kf joins, an intermediate table and a view table. We first insert records in both the base tables, read data from first base table and insert into first column family of an Intermediate table, and again read data from second base table and insert into second column family of an Intermediate table as explained in sections \ref{subsec:joinandaggregation} and \ref{subsec:joinandselection} respectively. After we have our Intermediate table ready, we apply k-kf join and insert data into view table accordingly.

\subsection{Control Parameter}
\label{Control Parameter}
There are certain control parameters defined for our experiments to determine performance.

\begin{description}
	\item[$\bullet$]  $noOfRegions$: The number of regions within a Region Server 
\end{description}

\begin{description}
	\item[$\bullet$]  $typesOfOperation$: The type of operation performed by the client. In our experiment, we've performed insert, update and delete operation. 
\end{description}

\begin{description}
	\item[$\bullet$]  $typesOfViews$: This parameter defines the types of views we have implemented in our experiments such as $Join$, $Selection$, $Sum$, $Count$, $Max$, $Min$. 
\end{description}

\begin{description}
	\item[$\bullet$]  $timeInMillis$: This parameter defines the time taken in milliseconds to perform certain operations. 
\end{description}

\section{Experiment 1 (Aggregation)}
\label{(sec:AggrExp)}
For $Aggregation$ view type, we have performed three different types of experiments. The first experiment $View$ $Re$-$computation$ $vs$ $Maintenance$ is performed on three different datasets on a single region and the remaining two experiments are performed on fixed dataset of 1.00.000 records with varying number of regions on a region server. 
 
\subsection{Aggregation: View Re-computation vs Maintenance}
In the first set, we insert 10.000 rows in the base table and we compute intermediate table and view tables accordingly. In the view table, we compute four different aggregation functions $Sum$, $Count$, $Min$ and $Max$. When client issues any update operation, either we re-compute view table from scratch and compute aggregation functions or we update view table incrementally. The graph shows time taken to re-compute view table vs time taken to update view table incrementally for each type of update operations. We conduct same experiment for 1.00.000 and 2.50.000 records. From the graph, we see that updating view table incrementally saves significant amount of time than re-computing view table for every client operation. The graph below shows time required in millisecond to recompute view table and incrementally update view table.


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Aggregation_exp_standalone}
	\caption{Aggregation Experiment Standalone}
	\label{sec:AggrExpStandalone}
	
\end{figure} 
\newpage
In the figure \ref{sec:AggrExpStandalone}, $Fig(a)$ shows the time taken to insert records in view table vs update view table incrementally whenever there is an update in the base table. This experiment is run in three different datasets and in each datasets we know that updating view table takes significantly less amount of time than recomputing view table. Fig(d) shows shows the performance metrics calculated in recomputing base table vs incremental maintenance. In the x-axis, we show types of client operations performed in base table and in y-axis we show performance matrix in percent. We calculate performance matrix using the formula,
\newline
\begin{equation}
\text{performance} = \frac{\text{time taken to compute view table} - \text{incremental update}}{\text{time taken to compute view table}} \times 100
\end{equation}
	 
One of the reasons for this performance optimization is that when recomputing a base table, we first have to perform read operation on base table and then write operation on view table. In incremental maintenance approach, only write operations is performed for every rows affected by the client operation on base table. The other reason is that the no. of rows updated in view table is less in comparison to the total no of rows in the base table.

\subsection{Aggregation: Insert Records}
\label{Insert Records Aggr}
We ran this experiment to see how it affects the result if we split our tables in multiple regions. We wanted to show that we can improve performance if we split our tables in multiple regions within a region server. In the figure \ref{sec:AggrExpStandalone}, $Fig(b)$ shows that if we split our table in multiple regions, we can reduce the insert operation in our tables. In x-axis we show no. of regions and in y-axis we show time taken in milliseconds to insert records in base table, intermediate table and view table. From the graph we see that, the more no. of regions the less time it takes to insert records. 

\subsection{Aggregation: Update View Table} 
\label{Update View Table Aggr}
In section \ref{Insert Records Aggr}, we proved that we can reduce insert operation time if we split our tables across multiple regions. In this experiment, we wanted to see how it affects our performance if we split $View$ $Table$ across multiple regions. In the figure \ref{sec:AggrExpStandalone}, $Fig(c)$ shows that we also can reduce operation time if we split $View$ $Table$ across multiple regions. In x-axis we show no. of regions and in y-axis we show time in milliseconds required to complete client operation. We ran this experiment on different types of client operations and for each operation we were able to decrease time taken to complete required operation. 

\section{Experiment 2 (Join and Aggregation)}
\label{(sec:Join and Aggregation Exp)} 
For $Join$ \& $Aggregation$, we create two base tables as it involves k-kf joins, an intermediate table and a view table. We insert 3.000 rows in both the base table, apply k-fk joins and calculate sum for those joins and insert result in the view table. We then run experiments to see how we can improve performance as we split our tables across multiple regions.

\subsection{Join and Aggregation: Insert records}
\label{Join and Aggregation: Insert records Standalone}
In this experiment, we insert 3.000 records in each of the base tables and apply k-fk join in the two base tables. First we run this experiment in a single region and see the number of rows generated in the view table after applying k-fk joins. We run this experiment for 2, 3 and 4 regions and split our tables accordingly. In the figure \ref{sec:JoinAggrExpStandalone}, Fig(a) shows the graph of no. of records inserted in view tables. We use this statistics for running other sub-experiments.

\subsection{Join and Aggregation: View Re-computation vs Maintenance}
\label{Join and Aggregation: View Re-computation vs Maintenance Standalone}
In this experiment, we wanted to show that by incrementally maintaining our view tables, we can increase the overall performance of the system. We ran this experiment on the basis of data gathered in sub-section \ref{Join and Aggregation: Insert records Standalone}. As a proof, we first ran this experiment on dataset of 1.53.761 records. From the graph \ref{sec:JoinAggrExpStandalone}, $Fig(b)$ shows the time required for re-computing view table vs time required to incrementally update view table. If we need to re-compute view table, in such scenario, we first need to perform read operation on the entire base table and then write operation in view table. Whereas in incremental maintenance, we only update the affected rows in view table. To ensure our hypothesis works, we ran our experiment on bigger dataset of 3.45.658 rows. From the experiment results, we proved that maintaining views incrementally is significantly less expensive than re-computing view table for every client request in base table.

\subsection{Join and Aggregation: Insert Records}
\label{Join and Aggregation: Insert Records Standalone}
We ran this experiment on the basis of data collected in section \ref{Join and Aggregation: Insert records Standalone}. In this experiment, we split our tables into multiple regions, and we want to show that splitting table across multiple regions enhances performance of the system. From the graph \ref{sec:JoinAggrExpStandalone}, $Fig(c)$ shows that we were able to optimize performance by splitting tables into multiple regions. In x-axis we show no. of regions and in y-axis we show time taken in milliseconds to insert records in base table, intermediate table and view table.

\subsection{Join and Aggregation: Update View Table} 
\label{Join and Aggr: Update View Table Standalone}
In section \ref{Join and Aggregation: Insert Records Standalone}, we showed that client operation time can be reduced significantly if we split our tables across multiple regions. In this section,we ran an experiment to see how we are able to optimize performance if we split $View$ $Table$ across multiple regions. For that, we split $View$ $Table$ across multiple regions and ran this experiment. From the graph \ref{sec:JoinAggrExpStandalone}, $Fig(d)$ proved that we also can reduce operation time if we split $View$ $Table$ across multiple regions. In x-axis we show no. of regions and in y-axis we show time in milliseconds required to complete client operation. We ran this experiment on different types of client operations and for each operation we were able to decrease time taken to complete required operation. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{JoinAndAggr_exp_standalone}
	\caption{Join and Aggregation Experiment Standalone}
	\label{sec:JoinAggrExpStandalone}
	
\end{figure} 
\newpage

\section{Experiment 3 (Join and Selection)}
\label{(sec:Join and Selection Exp Standalone)} 
For $Join$ \& $Selection$, we create two base tables as it also involves k-kf joins, an intermediate table and a view table. We insert 3.000 rows in both the base table, apply k-fk joins and select values for those joins and insert result in the view table. We then run experiments to see how we can improve performance as we split our tables across multiple regions.

\subsection{Join and Selection: Insert records}
\label{Join and Selection: Insert records Standalone}
In this experiment, we insert 3.000 records in each of the base tables and apply k-fk join in the two base tables. First we run this experiment in a single region and see the number of rows generated in the view table after applying k-fk joins. We run this experiment for 2, 3 and 4 regions and split our tables accordingly across those regions. In the figure \ref{sec:JoinSelExpStandalone}, Fig(a) shows the graph of no. of records inserted in view tables. We use this statistics for running other sub-experiments.

\subsection{Join and Selection: View Re-computation vs Maintenance}
\label{Join and Selection: View Re-computation vs Maintenance Standalone}
In this experiment, we wanted to show that by incrementally maintaining our view tables, we can increase the overall performance of the system. We ran this experiment on the basis of data gathered in sub-section \ref{Join and Selection: Insert records Standalone}. As a proof, we first ran this experiment on dataset of 1.53.561 records. From the graph \ref{sec:JoinSelExpStandalone}, $Fig(b)$ shows the time required for re-computing view table vs time required to incrementally update view table. If we need to re-compute view table, in such scenario, we first need to perform read operation on the entire base table and then write operation in view table. Whereas in incremental maintenance, we only update the affected rows in view table. To ensure our hypothesis works, we ran our experiment on bigger dataset of 3.45.049 rows. From the experiment results, we proved that maintaining views incrementally is significantly less expensive than re-computing view table for every client request in base table.

\subsection{Join and Selection: Insert Records}
\label{Join and Selection: Insert Records Standalone}
We ran this experiment on the basis of data collected in section \ref{Join and Selection: Insert records Standalone}. In this experiment, we split our tables into multiple regions, and we want to show that splitting table across multiple regions enhances performance of the system. From the graph \ref{sec:JoinSelExpStandalone}, $Fig(c)$ shows that we were able to optimize performance by splitting tables into multiple regions. In x-axis we show no. of regions and in y-axis we show time taken in milliseconds to insert records in base table, intermediate table and view table.

\subsection{Join and Selection: Update View Table} 
\label{Join and Sel: Update View Table Standalone}
In section \ref{Join and Selection: Insert Records Standalone}, we showed that client operation time can be reduced significantly if we split our tables across multiple regions. In this section,we ran an experiment to see how we are able to optimize performance if we split $View$ $Table$ across multiple regions. For that, we split $View$ $Table$ across multiple regions and ran this experiment. From the graph \ref{sec:JoinSelExpStandalone}, $Fig(d)$ proved that we also can reduce operation time if we split $View$ $Table$ across multiple regions. In x-axis we show no. of regions and in y-axis we show time in milliseconds required to complete client operation. We ran this experiment on different types of client operations and for each operation we were able to decrease time taken to complete required operation. 

\section{Experiment Setup (Fully Distributed Mode)}
In fully distributed mode, we have performed four different kinds of experiments for each of the three scenarios. First experiment $View$ $Re$-$computation$ $vs$ $Maintenance$ is performed on three different datasets while rest of the two experiments are performed on fixed dataset.

\subsection{Deployment}
We performed our experiment on a multi node cluster of 3 nodes, two of them are slave nodes and one is master node. We installed two virtual machines for the slave nodes and the master node running as a primary operating system. The master node is running on a computer server (Ubuntu 16.04 LTS, Intel Core i5-3230M CPU @ 2.60GHz x 4, 7.7GB RAM, 364.5GB HD). Both the slave nodes were running on a computer server (Ubuntu 16.04 LTS, Intel Core i5-3230M CPU @ 2.60GHz, 2.0GB RAM, 30.3GB HD). We installed hadoop in fully distributed mode. The services like JobTracker, TaskTracker, Namenode and Datanode runs as a separate Java process in a multiple cluster. We installed hadoop version 2.6.4 and hbase version 1.1.5 for our experiments. DataNode, TaskTracker runs on the slave machine whereas Namenode and JobTracker runs on the master. We have three Region servers, each running on master and slaves respectively.

\subsection{Table Configuration}
The table configuration for fully distributed is similar to pseudo distributed mode as described in sub-section \ref{Table Configuration standalone}.




\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{JoinAndSel_exp_standalone}
	\caption{Join and Selection Experiment Standalone}
	\label{sec:JoinSelExpStandalone}
	
\end{figure} 
\newpage

\subsection{Control Parameter}
\label{Control Parameter Distributed}
There are certain control parameters defined for our experiments to determine performance.

\begin{description}
	\item[$\bullet$]  $noOfRegionServers$: The number of Region Servers. Each Region Server can have one or many regions
\end{description}

\begin{description}
	\item[$\bullet$]  $noOfRegions$: The number of regions within a Region Server 
\end{description}

\begin{description}
	\item[$\bullet$]  $typesOfOperation$: The type of operation performed by the client. In our experiment, we've performed insert, update and delete operation. 
\end{description}

\begin{description}
	\item[$\bullet$]  $typesOfViews$: This parameter defines the types of views we have implemented in our experiments such as $Join$, $Selection$, $Sum$, $Count$, $Max$, $Min$. 
\end{description}

\begin{description}
	\item[$\bullet$]  $timeInMillis$: This parameter defines the time taken in milliseconds to perform certain operations. 
\end{description}

\section{Experiment 4 (Aggregation)}
\label{(sec:AggrExpDist)}
In this experiment, we have 

\newpage
\chapter{Discussion and Conclusion}

\newpage
\chapter{Future Work}


		% ---------------------------------------------------------------------------
		%
		% Appendix
		%
		% ---------------------------------------------------------------------------
		
		\part*{Appendix}
		\addcontentsline{toc}{part}{Appendix}
		
		\appendix %---------------------------------------
		
		%\input{chapters/oneAppendix}

  \clearemptydoublepage
  
	\bibliography{bibliography/literature}
	
 
\end{document}

